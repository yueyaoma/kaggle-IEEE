{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition Objective is to detect fraud in transactions; \n",
    "\n",
    "## Data\n",
    "\n",
    "\n",
    "In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n",
    "\n",
    "The data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n",
    "\n",
    "> Note: Not all transactions have corresponding identity information.\n",
    "\n",
    "**Categorical Features - Transaction**\n",
    "\n",
    "- ProductCD\n",
    "- emaildomain\n",
    "- card1 - card6\n",
    "- addr1, addr2\n",
    "- P_emaildomain\n",
    "- R_emaildomain\n",
    "- M1 - M9\n",
    "\n",
    "**Categorical Features - Identity**\n",
    "\n",
    "- DeviceType\n",
    "- DeviceInfo\n",
    "- id_12 - id_38\n",
    "\n",
    "**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n",
    "\n",
    "# Questions\n",
    "I will start exploring based on Categorical Features and Transaction Amounts.\n",
    "The aim is answer some questions like:\n",
    "- What type of data we have on our data?\n",
    "- How many cols, rows, missing values we have?\n",
    "- Whats the target distribution?\n",
    "- What's the Transactions values distribution of fraud and no fraud transactions?\n",
    "- We have predominant fraudulent products? \n",
    "- What features or target shows some interesting patterns? \n",
    "- And a lot of more questions that will raise trought the exploration. \n",
    "\n",
    "\n",
    "## I hope you enjoy my kernel and if it be useful for you, <b>upvote</b> the kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2025-06-10T09:53:31.828919Z",
     "iopub.status.busy": "2025-06-10T09:53:31.828628Z",
     "iopub.status.idle": "2025-06-10T09:53:31.852382Z",
     "shell.execute_reply": "2025-06-10T09:53:31.851532Z",
     "shell.execute_reply.started": "2025-06-10T09:53:31.828880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test_identity.csv', 'train_identity.csv', 'test_transaction.csv', 'train_transaction.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Standard plotly imports\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "#import cufflinks\n",
    "#import cufflinks as cf\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "#cufflinks.go_offline(connected=True)\n",
    "\n",
    "# Preprocessing, modelling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "## Hyperopt modules\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T09:53:31.855179Z",
     "iopub.status.busy": "2025-06-10T09:53:31.854789Z",
     "iopub.status.idle": "2025-06-10T09:53:54.036337Z",
     "shell.execute_reply": "2025-06-10T09:53:54.035738Z",
     "shell.execute_reply.started": "2025-06-10T09:53:31.855118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_id = pd.read_csv(\"../input/train_identity.csv\")\n",
    "df_trans = pd.read_csv(\"../input/train_transaction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will set all functions in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-06-10T09:53:54.038051Z",
     "iopub.status.busy": "2025-06-10T09:53:54.037760Z",
     "iopub.status.idle": "2025-06-10T09:53:54.057694Z",
     "shell.execute_reply": "2025-06-10T09:53:54.056967Z",
     "shell.execute_reply.started": "2025-06-10T09:53:54.037988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def resumetable(df):\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary = summary[['Name','dtypes']]\n",
    "    summary['Missing'] = df.isnull().sum().values    \n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['First Value'] = df.loc[0].values\n",
    "    summary['Second Value'] = df.loc[1].values\n",
    "    summary['Third Value'] = df.loc[2].values\n",
    "\n",
    "    for name in summary['Name'].value_counts().index:\n",
    "        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n",
    "\n",
    "    return summary\n",
    "\n",
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def CalcOutliers(df_num): \n",
    "\n",
    "    # calculating mean and std of the array\n",
    "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
    "\n",
    "    # seting the cut line to both higher and lower values\n",
    "    # You can change this value\n",
    "    cut = data_std * 3\n",
    "\n",
    "    #Calculating the higher and lower cut values\n",
    "    lower, upper = data_mean - cut, data_mean + cut\n",
    "\n",
    "    # creating an array of lower, higher and total outlier values \n",
    "    outliers_lower = [x for x in df_num if x < lower]\n",
    "    outliers_higher = [x for x in df_num if x > upper]\n",
    "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
    "\n",
    "    # array without outlier values\n",
    "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
    "    \n",
    "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
    "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
    "    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
    "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
    "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T09:53:54.059060Z",
     "iopub.status.busy": "2025-06-10T09:53:54.058839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## REducing memory\n",
    "df_trans = reduce_mem_usage(df_trans)\n",
    "df_id = reduce_mem_usage(df_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the output of the Resume Table, click to see the output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resumetable(df_trans)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\n",
    "total = len(df_trans)\n",
    "total_amt = df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "g = sns.countplot(x='isFraud', data=df_trans, )\n",
    "g.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\n",
    "g.set_xlabel(\"Is fraud?\", fontsize=18)\n",
    "g.set_ylabel('Count', fontsize=18)\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\", fontsize=15) \n",
    "\n",
    "perc_amt = (df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\n",
    "perc_amt = perc_amt.reset_index()\n",
    "plt.subplot(122)\n",
    "g1 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=perc_amt)\n",
    "g1.set_title(\"% Total Amount in Transaction Amt \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\n",
    "g1.set_xlabel(\"Is fraud?\", fontsize=18)\n",
    "g1.set_ylabel('Total Transaction Amount Scalar', fontsize=18)\n",
    "for p in g1.patches:\n",
    "    height = p.get_height()\n",
    "    g1.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total_amt * 100),\n",
    "            ha=\"center\", fontsize=15) \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3.5% of Fraud transactions in our dataset. <br>I think that it would be interesting to see if the amount percentual is higher or lower than 3.5% of total. I will see it later. <br>\n",
    "We have the same % when considering the Total Transactions Amount by Fraud and No Fraud. <br>\n",
    "Let's explore the Transaction amount further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Amount Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Ploting the Transaction Amount, let's see the quantiles of Transaction Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\n",
    "print(\"Transaction Amounts Quantiles:\")\n",
    "print(df_trans['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting Transaction Amount Values Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "plt.suptitle('Transaction Values Distribution', fontsize=22)\n",
    "plt.subplot(221)\n",
    "g = sns.distplot(df_trans[df_trans['TransactionAmt'] <= 1000]['TransactionAmt'])\n",
    "g.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\n",
    "g.set_xlabel(\"\")\n",
    "g.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.subplot(222)\n",
    "g1 = sns.distplot(np.log(df_trans['TransactionAmt']))\n",
    "g1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\n",
    "g1.set_xlabel(\"\")\n",
    "g1.set_ylabel(\"Probability\", fontsize=15)\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "\n",
    "plt.subplot(212)\n",
    "g4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n",
    "                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n",
    "                 label='NoFraud', alpha=.2)\n",
    "g4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]),\n",
    "                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n",
    "                 label='Fraud', alpha=.2)\n",
    "g4= plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\", fontsize=18)\n",
    "g4 = plt.xlabel(\"Index\")\n",
    "g4 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n",
    "g4 = plt.legend()\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.subplot(321)\n",
    "g = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]), \n",
    "                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n",
    "                label='isFraud', alpha=.4)\n",
    "plt.title(\"FRAUD - Transaction Amount ECDF\", fontsize=18)\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Amount Distribution\", fontsize=12)\n",
    "\n",
    "plt.subplot(322)\n",
    "g1 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n",
    "                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n",
    "                 label='NoFraud', alpha=.2)\n",
    "g1= plt.title(\"NO FRAUD - Transaction Amount ECDF\", fontsize=18)\n",
    "g1 = plt.xlabel(\"Index\")\n",
    "g1 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n",
    "\n",
    "plt.suptitle('Individual ECDF Distribution', fontsize=22)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now, we can see clearly the distribution of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeing the Quantiles of Fraud and No Fraud Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(pd.concat([df_trans[df_trans['isFraud'] == 1]['TransactionAmt']\\\n",
    "                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n",
    "                 .reset_index(), \n",
    "                 df_trans[df_trans['isFraud'] == 0]['TransactionAmt']\\\n",
    "                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n",
    "                 .reset_index()],\n",
    "                axis=1, keys=['Fraud', \"No Fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Amount Outliers\n",
    "- It's considering outlier values that are highest than 3 times the std from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CalcOutliers(df_trans['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider only values between >= 0 to 800 we will avoid the outliers and has more confidence in our distribution. <br>\n",
    "We have 10k rows with outliers that represents 1.74% of total rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "# Now, let's known the Product Feature\n",
    "- Distribution Products\n",
    "- Distribution of Frauds by Product\n",
    "- Has Difference between Transaction Amounts in Products? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.crosstab(df_trans['ProductCD'], df_trans['isFraud'], normalize='index') * 100\n",
    "tmp = tmp.reset_index()\n",
    "tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.suptitle('ProductCD Distributions', fontsize=22)\n",
    "\n",
    "plt.subplot(221)\n",
    "g = sns.countplot(x='ProductCD', data=df_trans)\n",
    "# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n",
    "\n",
    "g.set_title(\"ProductCD Distribution\", fontsize=19)\n",
    "g.set_xlabel(\"ProductCD Name\", fontsize=17)\n",
    "g.set_ylabel(\"Count\", fontsize=17)\n",
    "g.set_ylim(0,500000)\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\", fontsize=14) \n",
    "\n",
    "plt.subplot(222)\n",
    "g1 = sns.countplot(x='ProductCD', hue='isFraud', data=df_trans)\n",
    "plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n",
    "gt = g1.twinx()\n",
    "gt = sns.pointplot(x='ProductCD', y='Fraud', data=tmp, color='black', order=['W', 'H',\"C\", \"S\", \"R\"], legend=False)\n",
    "gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n",
    "\n",
    "g1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\n",
    "g1.set_xlabel(\"ProductCD Name\", fontsize=17)\n",
    "g1.set_ylabel(\"Count\", fontsize=17)\n",
    "\n",
    "plt.subplot(212)\n",
    "g3 = sns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n",
    "              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\n",
    "g3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\n",
    "g3.set_xlabel(\"ProductCD Name\", fontsize=17)\n",
    "g3.set_ylabel(\"Transaction Values\", fontsize=17)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.6, top = 0.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W, C and R are the most frequent values. <br>\n",
    "We can note that in W, H and R the distribution of Fraud values are slightly higher than the Non-Fraud Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Card Features\n",
    "- Based on Competition Description, card features are categoricals.\n",
    "- Lets understand the distribution of values\n",
    "- What's the different in transactions and % of Fraud for each values in these features\n",
    "- Card features has 6 columns, and 4 of them seems to be numericals, so lets see the quantiles and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Knowning the Card Features\n",
    "resumetable(df_trans[['card1', 'card2', 'card3','card4', 'card5', 'card6']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Card2-Card6 has some missing values. We will need to due with it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numericals Feature Card Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Card Features Quantiles: \")\n",
    "print(df_trans[['card1', 'card2', 'card3', 'card5']].quantile([0.01, .025, .1, .25, .5, .75, .975, .99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Card 1 and Card 2 has a large distribution of values, so maybe it will be better to get the log of these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_trans.loc[df_trans.card3.isin(df_trans.card3.value_counts()[df_trans.card3.value_counts() < 200].index), 'card3'] = \"Others\"\n",
    "df_trans.loc[df_trans.card5.isin(df_trans.card5.value_counts()[df_trans.card5.value_counts() < 300].index), 'card5'] = \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Card 1, Card 2 and Card 3 Distributions\n",
    "- As the Card 1 and 2 are numericals, I will plot the distribution of them\n",
    "- in Card 3, as we have many values with low frequencies, I decided to set value to \"Others\" \n",
    "- Also, in Card 3 I set the % of Fraud ratio in yaxis2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.crosstab(df_trans['card3'], df_trans['isFraud'], normalize='index') * 100\n",
    "tmp = tmp.reset_index()\n",
    "tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "tmp2 = pd.crosstab(df_trans['card5'], df_trans['isFraud'], normalize='index') * 100\n",
    "tmp2 = tmp2.reset_index()\n",
    "tmp2.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(14,22))\n",
    "\n",
    "plt.subplot(411)\n",
    "g = sns.distplot(df_trans[df_trans['isFraud'] == 1]['card1'], label='Fraud')\n",
    "g = sns.distplot(df_trans[df_trans['isFraud'] == 0]['card1'], label='NoFraud')\n",
    "g.legend()\n",
    "g.set_title(\"Card 1 Values Distribution by Target\", fontsize=20)\n",
    "g.set_xlabel(\"Card 1 Values\", fontsize=18)\n",
    "g.set_ylabel(\"Probability\", fontsize=18)\n",
    "\n",
    "plt.subplot(412)\n",
    "g1 = sns.distplot(df_trans[df_trans['isFraud'] == 1]['card2'].dropna(), label='Fraud')\n",
    "g1 = sns.distplot(df_trans[df_trans['isFraud'] == 0]['card2'].dropna(), label='NoFraud')\n",
    "g1.legend()\n",
    "g1.set_title(\"Card 2 Values Distribution by Target\", fontsize=20)\n",
    "g1.set_xlabel(\"Card 2 Values\", fontsize=18)\n",
    "g1.set_ylabel(\"Probability\", fontsize=18)\n",
    "\n",
    "plt.subplot(413)\n",
    "g2 = sns.countplot(x='card3', data=df_trans, order=list(tmp.card3.values))\n",
    "g22 = g2.twinx()\n",
    "gg2 = sns.pointplot(x='card3', y='Fraud', data=tmp, \n",
    "                    color='black', order=list(tmp.card3.values))\n",
    "gg2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n",
    "g2.set_title(\"Card 3 Values Distribution and % of Transaction Frauds\", fontsize=20)\n",
    "g2.set_xlabel(\"Card 3 Values\", fontsize=18)\n",
    "g2.set_ylabel(\"Count\", fontsize=18)\n",
    "for p in g2.patches:\n",
    "    height = p.get_height()\n",
    "    g2.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 25,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\") \n",
    "\n",
    "plt.subplot(414)\n",
    "g3 = sns.countplot(x='card5', data=df_trans, order=list(tmp2.card5.values))\n",
    "g3t = g3.twinx()\n",
    "g3t = sns.pointplot(x='card5', y='Fraud', data=tmp2, \n",
    "                    color='black', order=list(tmp2.card5.values))\n",
    "g3t.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n",
    "g3.set_title(\"Card 5 Values Distribution and % of Transaction Frauds\", fontsize=20)\n",
    "g3.set_xticklabels(g3.get_xticklabels(),rotation=90)\n",
    "g3.set_xlabel(\"Card 5 Values\", fontsize=18)\n",
    "g3.set_ylabel(\"Count\", fontsize=18)\n",
    "for p in g3.patches:\n",
    "    height = p.get_height()\n",
    "    g3.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\",fontsize=11) \n",
    "    \n",
    "plt.subplots_adjust(hspace = 0.6, top = 0.85)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool and Very Meaningful information. <br>\n",
    "In Card3 we can see that 100 and 106 are the most common values in the column. <br>\n",
    "We have 4.95% of Frauds in 100 and 1.52% in 106; The values with highest Fraud Transactions are 185, 119 and 119; <br>\n",
    "\n",
    "In card5 the most frequent values are 226, 224, 166 that represents 73% of data. Also is posible to see high % of frauds in 137, 147, 141 that has few entries for values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Card 4 - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.crosstab(df_trans['card4'], df_trans['isFraud'], normalize='index') * 100\n",
    "tmp = tmp.reset_index()\n",
    "tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.suptitle('Card 4 Distributions', fontsize=22)\n",
    "\n",
    "plt.subplot(221)\n",
    "g = sns.countplot(x='card4', data=df_trans)\n",
    "# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n",
    "g.set_title(\"Card4 Distribution\", fontsize=19)\n",
    "g.set_ylim(0,420000)\n",
    "g.set_xlabel(\"Card4 Category Names\", fontsize=17)\n",
    "g.set_ylabel(\"Count\", fontsize=17)\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\",fontsize=14) \n",
    "\n",
    "\n",
    "plt.subplot(222)\n",
    "g1 = sns.countplot(x='card4', hue='isFraud', data=df_trans)\n",
    "plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n",
    "gt = g1.twinx()\n",
    "gt = sns.pointplot(x='card4', y='Fraud', data=tmp, \n",
    "                   color='black', legend=False, \n",
    "                   order=['discover', 'mastercard', 'visa', 'american express'])\n",
    "gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n",
    "g1.set_title(\"Card4 by Target(isFraud)\", fontsize=19)\n",
    "g1.set_xlabel(\"Card4 Category Names\", fontsize=17)\n",
    "g1.set_ylabel(\"Count\", fontsize=17)\n",
    "\n",
    "plt.subplot(212)\n",
    "g3 = sns.boxenplot(x='card4', y='TransactionAmt', hue='isFraud', \n",
    "              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\n",
    "g3.set_title(\"Card 4 Distribuition by ProductCD and Target\", fontsize=20)\n",
    "g3.set_xlabel(\"Card4 Category Names\", fontsize=17)\n",
    "g3.set_ylabel(\"Transaction Values\", fontsize=17)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.6, top = 0.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 97% of our data are in Mastercard(32%) and Visa(65%);  <br>\n",
    "we have a highest value in discover(~8%) against ~3.5% of Mastercard and Visa and 2.87% in American Express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Card 6 - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tmp = pd.crosstab(df_trans['card6'], df_trans['isFraud'], normalize='index') * 100\n",
    "tmp = tmp.reset_index()\n",
    "tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.suptitle('Card 6 Distributions', fontsize=22)\n",
    "\n",
    "plt.subplot(221)\n",
    "g = sns.countplot(x='card6', data=df_trans, order=list(tmp.card6.values))\n",
    "# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n",
    "g.set_title(\"Card6 Distribution\", fontsize=19)\n",
    "g.set_ylim(0,480000)\n",
    "g.set_xlabel(\"Card6 Category Names\", fontsize=17)\n",
    "g.set_ylabel(\"Count\", fontsize=17)\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\",fontsize=14) \n",
    "\n",
    "plt.subplot(222)\n",
    "g1 = sns.countplot(x='card6', hue='isFraud', data=df_trans, order=list(tmp.card6.values))\n",
    "plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n",
    "gt = g1.twinx()\n",
    "gt = sns.pointplot(x='card6', y='Fraud', data=tmp, order=list(tmp.card6.values),\n",
    "                   color='black', legend=False, )\n",
    "gt.set_ylim(0,20)\n",
    "gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n",
    "g1.set_title(\"Card6 by Target(isFraud)\", fontsize=19)\n",
    "g1.set_xlabel(\"Card6 Category Names\", fontsize=17)\n",
    "g1.set_ylabel(\"Count\", fontsize=17)\n",
    "\n",
    "plt.subplot(212)\n",
    "g3 = sns.boxenplot(x='card6', y='TransactionAmt', hue='isFraud', order=list(tmp.card6.values),\n",
    "              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\n",
    "g3.set_title(\"Card 6 Distribuition by ProductCD and Target\", fontsize=20)\n",
    "g3.set_xlabel(\"Card6 Category Names\", fontsize=17)\n",
    "g3.set_ylabel(\"Transaction Values\", fontsize=17)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.6, top = 0.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data is on Credit and Debit. We can see a high percentual of Frauds in Credit than Debit transactions. <br>\n",
    "The Distribution of Transaction Amount don't shows clear differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring M1-M9 Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n",
    "    df_trans[col] = df_trans[col].fillna(\"Miss\")\n",
    "    \n",
    "def ploting_dist_ratio(df, col, lim=2000):\n",
    "    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.suptitle(f'{col} Distributions ', fontsize=22)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    g = sns.countplot(x=col, data=df, order=list(tmp[col].values))\n",
    "    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n",
    "    g.set_title(f\"{col} Distribution\\nCound and %Fraud by each category\", fontsize=18)\n",
    "    g.set_ylim(0,400000)\n",
    "    gt = g.twinx()\n",
    "    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n",
    "                       color='black', legend=False, )\n",
    "    gt.set_ylim(0,20)\n",
    "    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n",
    "    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n",
    "    g.set_ylabel(\"Count\", fontsize=17)\n",
    "    for p in gt.patches:\n",
    "        height = p.get_height()\n",
    "        gt.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(height/total*100),\n",
    "                ha=\"center\",fontsize=14) \n",
    "        \n",
    "    perc_amt = (df_trans.groupby(['isFraud',col])['TransactionAmt'].sum() / total_amt * 100).unstack('isFraud')\n",
    "    perc_amt = perc_amt.reset_index()\n",
    "    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    g1 = sns.boxplot(x=col, y='TransactionAmt', hue='isFraud', \n",
    "                     data=df[df['TransactionAmt'] <= lim], order=list(tmp[col].values))\n",
    "    g1t = g1.twinx()\n",
    "    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, order=list(tmp[col].values),\n",
    "                       color='black', legend=False, )\n",
    "    g1t.set_ylim(0,5)\n",
    "    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n",
    "    g1.set_title(f\"{col} by Transactions dist\", fontsize=18)\n",
    "    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n",
    "    g1.set_ylabel(\"Transaction Amount(U$)\", fontsize=16)\n",
    "        \n",
    "    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M distributions:  Count, %Fraud and Transaction Amount distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n",
    "    ploting_dist_ratio(df_trans, col, lim=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool!!! This graphs give us many interesting intuition about the M features.<br>\n",
    "Only in M4 the Missing values haven't the highest % of Fraud.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addr1 and Addr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Card Features Quantiles: \")\n",
    "print(df_trans[['addr1', 'addr2']].quantile([0.01, .025, .1, .25, .5, .75, .90,.975, .99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will set all values in Addr1 that has less than 5000 entries to \"Others\"<br>\n",
    "In Addr2 I will set as \"Others\" all values with less than 50 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans.loc[df_trans.addr1.isin(df_trans.addr1.value_counts()[df_trans.addr1.value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\n",
    "df_trans.loc[df_trans.addr2.isin(df_trans.addr2.value_counts()[df_trans.addr2.value_counts() <= 50 ].index), 'addr2'] = \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addr1 Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    " def ploting_cnt_amt(df, col, lim=2000):\n",
    "    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "    \n",
    "    plt.figure(figsize=(16,14))    \n",
    "    plt.suptitle(f'{col} Distributions ', fontsize=24)\n",
    "    \n",
    "    plt.subplot(211)\n",
    "    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n",
    "    gt = g.twinx()\n",
    "    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n",
    "                       color='black', legend=False, )\n",
    "    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n",
    "    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n",
    "    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n",
    "    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n",
    "    g.set_ylabel(\"Count\", fontsize=17)\n",
    "    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n",
    "    sizes = []\n",
    "    for p in g.patches:\n",
    "        height = p.get_height()\n",
    "        sizes.append(height)\n",
    "        g.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(height/total*100),\n",
    "                ha=\"center\",fontsize=12) \n",
    "        \n",
    "    g.set_ylim(0,max(sizes)*1.15)\n",
    "    \n",
    "    #########################################################################\n",
    "    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n",
    "                / df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n",
    "    perc_amt = perc_amt.reset_index()\n",
    "    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n",
    "    perc_amt = perc_amt.fillna(0)\n",
    "    plt.subplot(212)\n",
    "    g1 = sns.barplot(x=col, y='TransactionAmt', \n",
    "                       data=amt, \n",
    "                       order=list(tmp[col].values))\n",
    "    g1t = g1.twinx()\n",
    "    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n",
    "                        order=list(tmp[col].values),\n",
    "                       color='black', legend=False, )\n",
    "    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n",
    "    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n",
    "    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n",
    "    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n",
    "    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n",
    "    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n",
    "    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n",
    "    \n",
    "    for p in g1.patches:\n",
    "        height = p.get_height()\n",
    "        g1.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(height/total_amt*100),\n",
    "                ha=\"center\",fontsize=12) \n",
    "        \n",
    "    plt.subplots_adjust(hspace=.4, top = 0.9)\n",
    "    plt.show()\n",
    "    \n",
    "ploting_cnt_amt(df_trans, 'addr1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can note interesting patterns on Addr1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addr2 Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, 'addr2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all entries in Addr2 are in the same value. <br>\n",
    "Interestingly in the value 65 , the percent of frauds are almost 60% <br>\n",
    "Altought the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P emaildomain Distributions\n",
    "- I will group all e-mail domains by the respective enterprises.\n",
    "- Also, I will set as \"Others\" all values with less than 500 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans.loc[df_trans['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n",
    "\n",
    "df_trans.loc[df_trans['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n",
    "                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n",
    "                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\n",
    "df_trans.loc[df_trans['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n",
    "                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n",
    "                                         'outlook.es', 'live.com', 'live.fr',\n",
    "                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\n",
    "df_trans.loc[df_trans.P_emaildomain.isin(df_trans.P_emaildomain\\\n",
    "                                         .value_counts()[df_trans.P_emaildomain.value_counts() <= 500 ]\\\n",
    "                                         .index), 'P_emaildomain'] = \"Others\"\n",
    "df_trans.P_emaildomain.fillna(\"NoInf\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting P-Email Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, 'P_emaildomain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R-Email Domain plot distribution\n",
    "- I will group all e-mail domains by the respective enterprises.\n",
    "- I will set as \"Others\" all values with less than 300 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans.loc[df_trans['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n",
    "\n",
    "df_trans.loc[df_trans['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n",
    "                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n",
    "                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\n",
    "df_trans.loc[df_trans['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n",
    "                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n",
    "                                             'outlook.es', 'live.com', 'live.fr',\n",
    "                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\n",
    "df_trans.loc[df_trans.R_emaildomain.isin(df_trans.R_emaildomain\\\n",
    "                                         .value_counts()[df_trans.R_emaildomain.value_counts() <= 300 ]\\\n",
    "                                         .index), 'R_emaildomain'] = \"Others\"\n",
    "df_trans.R_emaildomain.fillna(\"NoInf\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, 'R_emaildomain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a very similar distribution in both email domain features. <br>\n",
    "It's interesting that we have high values in google and icloud frauds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1-C14 features\n",
    "- Let's understand what this features are.\n",
    "- What's the distributions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resumetable(df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n",
    "                      'C9', 'C10', 'C11', 'C12', 'C13', 'C14']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n",
    "                      'C9', 'C10', 'C11', 'C12', 'C13', 'C14']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans.loc[df_trans.C1.isin(df_trans.C1\\\n",
    "                              .value_counts()[df_trans.C1.value_counts() <= 400 ]\\\n",
    "                              .index), 'C1'] = \"Others\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C1 Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, 'C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans.loc[df_trans.C2.isin(df_trans.C2\\\n",
    "                              .value_counts()[df_trans.C2.value_counts() <= 350 ]\\\n",
    "                              .index), 'C2'] = \"Others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, 'C2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeDelta Feature\n",
    "- Let's see if the frauds have some specific hour that has highest % of frauds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Total Days, Weekdays and Hours\n",
    "In discussions tab I read an excellent solution to Timedelta column, I will set the link below; <br>\n",
    "We will use the first date as 2017-12-01 and use the delta time to compute datetime features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100400#latest-579480\n",
    "import datetime\n",
    "\n",
    "START_DATE = '2017-12-01'\n",
    "startdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n",
    "df_trans[\"Date\"] = df_trans['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n",
    "\n",
    "df_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek\n",
    "df_trans['_Hours'] = df_trans['Date'].dt.hour\n",
    "df_trans['_Days'] = df_trans['Date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Days with highest Total Transaction Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, '_Days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting WeekDays Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, '_Weekdays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have the reference of date but we can see that two days has lower transactions, that we can infer it is weekend days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting Hours Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_trans, '_Hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions and Total Amount by each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calling the function to transform the date column in datetime pandas object\n",
    "\n",
    "#seting some static color options\n",
    "color_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n",
    "            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n",
    "            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n",
    "\n",
    "\n",
    "dates_temp = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].count().reset_index()\n",
    "# renaming the columns to apropriate names\n",
    "\n",
    "# creating the first trace with the necessary parameters\n",
    "trace = go.Scatter(x=dates_temp['Date'], y=dates_temp.TransactionAmt,\n",
    "                    opacity = 0.8, line = dict(color = color_op[7]), name= 'Total Transactions')\n",
    "\n",
    "# Below we will get the total amount sold\n",
    "dates_temp_sum = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].sum().reset_index()\n",
    "\n",
    "# using the new dates_temp_sum we will create the second trace\n",
    "trace1 = go.Scatter(x=dates_temp_sum.Date, line = dict(color = color_op[1]), name=\"Total Amount\",\n",
    "                        y=dates_temp_sum['TransactionAmt'], opacity = 0.8, yaxis='y2')\n",
    "\n",
    "#creating the layout the will allow us to give an title and \n",
    "# give us some interesting options to handle with the outputs of graphs\n",
    "layout = dict(\n",
    "    title= \"Total Transactions and Fraud Informations by Date\",\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label='1m', step='month', stepmode='backward'),\n",
    "                dict(count=3, label='3m', step='month', stepmode='backward'),\n",
    "                dict(count=6, label='6m', step='month', stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(visible = True),\n",
    "        type='date' ),\n",
    "    yaxis=dict(title='Total Transactions'),\n",
    "    yaxis2=dict(overlaying='y',\n",
    "                anchor='x', side='right',\n",
    "                zeroline=False, showgrid=False,\n",
    "                title='Total Transaction Amount')\n",
    ")\n",
    "\n",
    "# creating figure with the both traces and layout\n",
    "fig = dict(data= [trace, trace1,], layout=layout)\n",
    "\n",
    "#rendering the graphs\n",
    "iplot(fig) #it's an equivalent to plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FRAUD TRANSACTIONS BY DATE\n",
    "- Visualizing only Fraud Transactions by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calling the function to transform the date column in datetime pandas object\n",
    "\n",
    "#seting some static color options\n",
    "color_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n",
    "            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n",
    "            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n",
    "\n",
    "tmp_amt = df_trans.groupby([df_trans.Date.dt.date, 'isFraud'])['TransactionAmt'].sum().reset_index()\n",
    "tmp_trans = df_trans.groupby([df_trans.Date.dt.date, 'isFraud'])['TransactionAmt'].count().reset_index()\n",
    "\n",
    "tmp_trans_fraud = tmp_trans[tmp_trans['isFraud'] == 1]\n",
    "tmp_amt_fraud = tmp_amt[tmp_amt['isFraud'] == 1]\n",
    "\n",
    "dates_temp = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].count().reset_index()\n",
    "# renaming the columns to apropriate names\n",
    "\n",
    "# creating the first trace with the necessary parameters\n",
    "trace = go.Scatter(x=tmp_trans_fraud['Date'], y=tmp_trans_fraud.TransactionAmt,\n",
    "                    opacity = 0.8, line = dict(color = color_op[1]), name= 'Fraud Transactions')\n",
    "\n",
    "# using the new dates_temp_sum we will create the second trace\n",
    "trace1 = go.Scatter(x=tmp_amt_fraud.Date, line = dict(color = color_op[7]), name=\"Fraud Amount\",\n",
    "                    y=tmp_amt_fraud['TransactionAmt'], opacity = 0.8, yaxis='y2')\n",
    "\n",
    "#creating the layout the will allow us to give an title and \n",
    "# give us some interesting options to handle with the outputs of graphs\n",
    "layout = dict(\n",
    "    title= \"FRAUD TRANSACTIONS - Total Transactions and Fraud Informations by Date\",\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label='1m', step='month', stepmode='backward'),\n",
    "                dict(count=3, label='3m', step='month', stepmode='backward'),\n",
    "                dict(count=6, label='6m', step='month', stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(visible = True),\n",
    "        type='date' ),\n",
    "    yaxis=dict(title='Total Transactions'),\n",
    "    yaxis2=dict(overlaying='y',\n",
    "                anchor='x', side='right',\n",
    "                zeroline=False, showgrid=False,\n",
    "                title='Total Transaction Amount')\n",
    ")\n",
    "\n",
    "# creating figure with the both traces and layout\n",
    "fig = dict(data= [trace, trace1], layout=layout)\n",
    "\n",
    "#rendering the graphs\n",
    "iplot(fig) #it's an equivalent to plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features [id_12 to id_38]\n",
    "- categorical features in training identity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_id[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n",
    "       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n",
    "       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n",
    "       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cat_feat_ploting(df, col):\n",
    "    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(14,10))\n",
    "    plt.suptitle(f'{col} Distributions', fontsize=22)\n",
    "\n",
    "    plt.subplot(221)\n",
    "    g = sns.countplot(x=col, data=df, order=tmp[col].values)\n",
    "    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n",
    "\n",
    "    g.set_title(f\"{col} Distribution\", fontsize=19)\n",
    "    g.set_xlabel(f\"{col} Name\", fontsize=17)\n",
    "    g.set_ylabel(\"Count\", fontsize=17)\n",
    "    # g.set_ylim(0,500000)\n",
    "    for p in g.patches:\n",
    "        height = p.get_height()\n",
    "        g.text(p.get_x()+p.get_width()/2.,\n",
    "                height + 3,\n",
    "                '{:1.2f}%'.format(height/total*100),\n",
    "                ha=\"center\", fontsize=14) \n",
    "\n",
    "    plt.subplot(222)\n",
    "    g1 = sns.countplot(x=col, hue='isFraud', data=df, order=tmp[col].values)\n",
    "    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n",
    "    gt = g1.twinx()\n",
    "    gt = sns.pointplot(x=col, y='Fraud', data=tmp, color='black', order=tmp[col].values, legend=False)\n",
    "    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n",
    "\n",
    "    g1.set_title(f\"{col} by Target(isFraud)\", fontsize=19)\n",
    "    g1.set_xlabel(f\"{col} Name\", fontsize=17)\n",
    "    g1.set_ylabel(\"Count\", fontsize=17)\n",
    "\n",
    "    plt.subplot(212)\n",
    "    g3 = sns.boxenplot(x=col, y='TransactionAmt', hue='isFraud', \n",
    "                       data=df[df['TransactionAmt'] <= 2000], order=tmp[col].values )\n",
    "    g3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\n",
    "    g3.set_xlabel(\"ProductCD Name\", fontsize=17)\n",
    "    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n",
    "\n",
    "    plt.subplots_adjust(hspace = 0.4, top = 0.85)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting columns with few unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for col in ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n",
    "    df_train[col] = df_train[col].fillna('NaN')\n",
    "    cat_feat_ploting(df_train, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Id 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.loc[df_train['id_30'].str.contains('Windows', na=False), 'id_30'] = 'Windows'\n",
    "df_train.loc[df_train['id_30'].str.contains('iOS', na=False), 'id_30'] = 'iOS'\n",
    "df_train.loc[df_train['id_30'].str.contains('Mac OS', na=False), 'id_30'] = 'Mac'\n",
    "df_train.loc[df_train['id_30'].str.contains('Android', na=False), 'id_30'] = 'Android'\n",
    "df_train['id_30'].fillna(\"NAN\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_train, 'id_30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Id 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.loc[df_train['id_31'].str.contains('chrome', na=False), 'id_31'] = 'Chrome'\n",
    "df_train.loc[df_train['id_31'].str.contains('firefox', na=False), 'id_31'] = 'Firefox'\n",
    "df_train.loc[df_train['id_31'].str.contains('safari', na=False), 'id_31'] = 'Safari'\n",
    "df_train.loc[df_train['id_31'].str.contains('edge', na=False), 'id_31'] = 'Edge'\n",
    "df_train.loc[df_train['id_31'].str.contains('ie', na=False), 'id_31'] = 'IE'\n",
    "df_train.loc[df_train['id_31'].str.contains('samsung', na=False), 'id_31'] = 'Samsung'\n",
    "df_train.loc[df_train['id_31'].str.contains('opera', na=False), 'id_31'] = 'Opera'\n",
    "df_train['id_31'].fillna(\"NAN\", inplace=True)\n",
    "df_train.loc[df_train.id_31.isin(df_train.id_31.value_counts()[df_train.id_31.value_counts() < 200].index), 'id_31'] = \"Others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ploting_cnt_amt(df_train, 'id_31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling \n",
    "To start simple, I will start using as base the kernels below: <br>\n",
    "https://www.kaggle.com/artkulak/ieee-fraud-simple-baseline-0-9383-lb - (@artkulak - Art) <br>\n",
    "https://www.kaggle.com/artgor/eda-and-models - (@artgor - Andrew Lukyanenko)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_trans = pd.read_csv('../input/train_transaction.csv')\n",
    "df_test_trans = pd.read_csv('../input/test_transaction.csv')\n",
    "\n",
    "df_id = pd.read_csv('../input/train_identity.csv')\n",
    "df_test_id = pd.read_csv('../input/test_identity.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "df_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# y_train = df_train['isFraud'].copy()\n",
    "del df_trans, df_id, df_test_trans, df_test_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "# https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    df_train[c + '_bin'] = df_train[c].map(emails)\n",
    "    df_test[c + '_bin'] = df_test[c].map(emails)\n",
    "    \n",
    "    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if f in df_test.columns:\n",
    "        if df_train[f].dtype == 'object' or df_test[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(df_train[f].astype(str).values) + list(df_test[f].astype(str).values))\n",
    "            df_train[f] = lbl.transform(list(df_train[f].astype(str).values))\n",
    "            df_test[f] = lbl.transform(list(df_test[f].astype(str).values))\n",
    "    else:\n",
    "        if df_train[f].dtype == 'object':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(df_train[f].astype(str).values))\n",
    "            df_train[f] = lbl.transform(list(df_train[f].astype(str).values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "trusted": true
   },
   "source": [
    "# Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\n",
    "df_train['Trans_min_std'] = df_train['Trans_min_mean'] / df_train['TransactionAmt'].std()\n",
    "df_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\n",
    "df_test['Trans_min_std'] = df_test['Trans_min_mean'] / df_test['TransactionAmt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "df_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "df_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "df_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "df_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "df_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "df_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "df_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\n",
    "df_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating dfs to get PCA of V features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test['isFraud'] = 'test'\n",
    "df = pd.concat([df_train, df_test], axis=0, sort=False )\n",
    "df = df.reset_index()\n",
    "df = df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n",
    "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
    "\n",
    "    principalComponents = pca.fit_transform(df[cols])\n",
    "\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mas_v = df_train.columns[55:394]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "for col in mas_v:\n",
    "    df[col] = df[col].fillna((df[col].min() - 2))\n",
    "    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
    "\n",
    "    \n",
    "df = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seting train and test back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seting X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n",
    "                                                      'TransactionDT', \n",
    "                                                      #'Card_ID'\n",
    "                                                     ],\n",
    "                                                     axis=1)\n",
    "y_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n",
    "\n",
    "X_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n",
    "                                                    #'Card_ID'\n",
    "                                                   ], \n",
    "                                                   axis=1)\n",
    "del df_train\n",
    "df_test = df_test[[\"TransactionDT\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the HyperOpt function with parameters space and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "import time\n",
    "def objective(params):\n",
    "    time1 = time.time()\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "    }\n",
    "\n",
    "    print(\"\\n############## New Run ################\")\n",
    "    print(f\"params = {params}\")\n",
    "    FOLDS = 7\n",
    "    count=1\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "    y_preds = np.zeros(sample_submission.shape[0])\n",
    "    y_oof = np.zeros(X_train.shape[0])\n",
    "    score_mean = 0\n",
    "    for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "        clf = xgb.XGBClassifier(\n",
    "            n_estimators=600, random_state=4, verbose=True, \n",
    "            tree_method='gpu_hist', \n",
    "            **params\n",
    "        )\n",
    "\n",
    "        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "        #print(y_pred_train)\n",
    "        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "        # plt.show()\n",
    "        score_mean += score\n",
    "        print(f'{count} CV - score: {round(score, 4)}')\n",
    "        count += 1\n",
    "    time2 = time.time() - time1\n",
    "    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "    gc.collect()\n",
    "    print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "    del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "    return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "space = {\n",
    "    # The maximum depth of a tree, same as GBM.\n",
    "    # Used to control over-fitting as higher depth will allow model \n",
    "    # to learn relations very specific to a particular sample.\n",
    "    # Should be tuned using CV.\n",
    "    # Typical values: 3-10\n",
    "    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "    \n",
    "    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "    # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "    # is logistic regression since you might need help with feature selection.\n",
    "    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "    # approach can be more useful in tree-models where zeroing \n",
    "    # features might not make much sense.\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "    # eta: Analogous to learning rate in GBM\n",
    "    # Makes the model more robust by shrinking the weights on each step\n",
    "    # Typical final values to be used: 0.01-0.2\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    \n",
    "    # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "    # fraction of columns to be randomly samples for each tree.\n",
    "    # Typical values: 0.5-1\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "    # A node is split only when the resulting split gives a positive\n",
    "    # reduction in the loss function. Gamma specifies the \n",
    "    # minimum loss reduction required to make a split.\n",
    "    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "    'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "    # more increases accuracy, but may lead to overfitting.\n",
    "    # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "    # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "    # specifies the minimum samples per leaf node.\n",
    "    # the minimum number of samples (data) to group into a leaf. \n",
    "    # The parameter can greatly assist with overfitting: larger sample\n",
    "    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "    # subsample: represents a fraction of the rows (observations) to be \n",
    "    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "    # in their paper A Scalable Tree Boosting System recommend \n",
    "    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "    # randomly select a fraction of the features.\n",
    "    # feature_fraction: controls the subsampling of features used\n",
    "    # for training (as opposed to subsampling the actual training data in \n",
    "    # the case of bagging). Smaller fractions reduce overfitting.\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "    # randomly bag or subsample training data.\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "    # of the training data. Both values need to be set for bagging to be used.\n",
    "    # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "    # fractions and frequencies reduce overfitting.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set algoritm parameters\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=27)\n",
    "\n",
    "# Print best parameters\n",
    "best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"BEST PARAMS: \", best_params)\n",
    "\n",
    "best_params['max_depth'] = int(best_params['max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning and Predicting with best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting X test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    **best_params,\n",
    "    tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_preds = clf.predict_proba(X_test)[:,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_important = clf.get_booster().get_score(importance_type=\"weight\")\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "\n",
    "# Top 10 features\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting y_pred to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sample_submission['isFraud'] = y_preds\n",
    "sample_submission.to_csv('XGB_hypopt_model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I'm working in this kernel yet.\n",
    "# <font color=\"red\">Please if this kernel were useful for you, please <b>UPVOTE</b> =)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 568274,
     "sourceId": 14242,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 28772,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
